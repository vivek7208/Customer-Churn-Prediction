{"cells":[{"cell_type":"markdown","id":"9b08c378","metadata":{},"source":["# Churn Prediction for Mobile Phone Customers with XGBoost Demo\n","_**Using Gradient Boosted Trees to Predict Mobile Phone Customer Departure**_\n","\n","---\n","In this demo notebook, we can quickly send some data to an already deployed endpoint, get the response and visualize its results.\n","\n","**To see more details of end-to-end model training with hyper-parameter optimization and deployement using SageMaker, please click [xgboost_customer_churn.ipynb](./xgboost_customer_churn.ipynb) notebook.**\n","\n","---\n","\n","## Runtime\n","\n","This notebook takes less than a minute to run.\n","\n","## Contents\n","\n","1. Background\n","1. Setup\n","1. Data\n","1. Using the endpoint\n","    1. Evaluate\n","1. Extensions\n","\n","---\n","\n","## Background\n","\n","_This notebook has been adapted from an [AWS blog post](https://aws.amazon.com/blogs/ai/predicting-customer-churn-with-amazon-machine-learning/)_\n","\n","Losing customers is costly for any business.  Identifying unhappy customers early on gives you a chance to offer them incentives to stay.  This notebook describes using machine learning (ML) for the automated identification of unhappy customers, also known as customer churn prediction. ML models rarely give perfect predictions though, so this notebook is also about how to incorporate the relative costs of prediction mistakes when determining the financial outcome of using ML.\n","\n","We use a familiar example of churn: leaving a mobile phone operator.  Seems like one can always find fault with their provider du jour! And if the provider knows that a customer is thinking of leaving, it can offer timely incentives - such as a phone upgrade or perhaps having a new feature activated – and the customer may stick around. Incentives are often much more cost-effective than losing and reacquiring a customer.\n","\n","---\n","\n","## Setup\n","\n","Let's start by updating the required packages i.e. SageMaker Python SDK, `pandas` and `numpy`, and specifying:\n","\n","- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance or Studio, training, and hosting.\n","- The IAM role ARN used to give training and hosting access to your data. See the documentation for how to create these.  Note: if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with the appropriate full IAM role ARN string(s)."]},{"cell_type":"code","execution_count":null,"id":"4f00baad","metadata":{},"outputs":[],"source":["import sys\n","\n","!{sys.executable} -m pip install sagemaker pandas numpy --upgrade --quiet"]},{"cell_type":"markdown","id":"872ef103","metadata":{},"source":["This solution relies on a config file to run the provisioned AWS resources. Run the cell below to generate that file."]},{"cell_type":"code","execution_count":null,"id":"a6cf448e","metadata":{},"outputs":[],"source":["import boto3\n","import os\n","import json"]},{"cell_type":"code","execution_count":null,"id":"279755ef","metadata":{},"outputs":[],"source":["client = boto3.client('servicecatalog')\n","cwd = os.getcwd().split('/')\n","i= cwd.index('S3Downloads')\n","pp_name = cwd[i + 1]\n","pp = client.describe_provisioned_product(Name=pp_name)\n","record_id = pp['ProvisionedProductDetail']['LastSuccessfulProvisioningRecordId']\n","record = client.describe_record(Id=record_id)\n","\n","keys = [ x['OutputKey'] for x in record['RecordOutputs'] if 'OutputKey' and 'OutputValue' in x]\n","values = [ x['OutputValue'] for x in record['RecordOutputs'] if 'OutputKey' and 'OutputValue' in x]\n","stack_output = dict(zip(keys, values))\n","\n","with open(f'/root/S3Downloads/{pp_name}/stack_outputs.json', 'w') as f:\n","    json.dump(stack_output, f)"]},{"cell_type":"code","execution_count":null,"id":"e4c1b3c0","metadata":{"isConfigCell":true,"tags":["parameters"]},"outputs":[],"source":["import sagemaker\n","\n","sess = sagemaker.Session()\n","\n","# Get config\n","sagemaker_config = json.load(open(\"stack_outputs.json\"))\n","endpoint_name = sagemaker_config[\"DemoEndpointName\"]\n","solution_bucket = sagemaker_config[\"SolutionS3Bucket\"]\n","region = sagemaker_config[\"AWSRegion\"]\n","library_version = sagemaker_config[\"LibraryVersion\"]\n","solution_name = sagemaker_config[\"SolutionName\"]\n","source_bucket = f\"s3://{solution_bucket}-{region}/{library_version}/{solution_name}\"\n","data_prefix = f\"artifacts/data\""]},{"cell_type":"markdown","id":"e02e6dbb","metadata":{"tags":[]},"source":["Next, we'll import the Python libraries we'll need for the remainder of the example."]},{"cell_type":"code","execution_count":null,"id":"08714702","metadata":{"papermill":{"duration":0.666347,"end_time":"2021-06-07T00:09:46.367361","exception":false,"start_time":"2021-06-07T00:09:45.701014","status":"completed"},"tags":[]},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sagemaker.serializers import CSVSerializer"]},{"cell_type":"markdown","id":"6c810d34","metadata":{"tags":[]},"source":["---\n","## Data\n","\n","Mobile operators have historical records on which customers ultimately ended up churning and which continued using the service. We can use this historical information to construct an ML model of one mobile operator’s churn using a process called training. After training the model, we can pass the profile information of an arbitrary customer (the same profile information that we used to train the model) to the model, and have the model predict whether this customer is going to churn. Of course, we expect the model to make mistakes. After all, predicting the future is tricky business! But we'll learn how to deal with prediction errors.\n","\n","The dataset we use is publicly available and was mentioned in the book [Discovering Knowledge in Data](https://www.amazon.com/dp/0470908742/) by Daniel T. Larose. It is attributed by the author to the University of California Irvine Repository of Machine Learning Datasets. Let's read that dataset in now:"]},{"cell_type":"code","execution_count":null,"id":"9fca2b1a","metadata":{},"outputs":[],"source":["DATASET_NAME = \"churn.txt\"\n","data_source = f\"{source_bucket}/{data_prefix}/{DATASET_NAME}\"\n","print(\"original data: \")\n","!aws s3 cp $data_source ."]},{"cell_type":"code","execution_count":null,"id":"b89ecb3f","metadata":{"tags":[]},"outputs":[],"source":["churn = pd.read_csv(\"churn.txt\")\n","churn.head(5)"]},{"cell_type":"code","execution_count":null,"id":"2d3c3733","metadata":{},"outputs":[],"source":["len(churn.columns)"]},{"cell_type":"markdown","id":"a1380adb","metadata":{"tags":[]},"source":["By modern standards, it’s a relatively small dataset, with only 5,000 records, where each record uses 21 attributes to describe the profile of a customer of an unknown US mobile operator. The attributes are:\n","\n","- `State`: the US state in which the customer resides, indicated by a two-letter abbreviation; for example, OH or NJ\n","- `Account Length`: the number of days that this account has been active\n","- `Area Code`: the three-digit area code of the corresponding customer’s phone number\n","- `Phone`: the remaining seven-digit phone number\n","- `Int’l Plan`: whether the customer has an international calling plan: yes/no\n","- `VMail Plan`: whether the customer has a voice mail feature: yes/no\n","- `VMail Message`: the average number of voice mail messages per month\n","- `Day Mins`: the total number of calling minutes used during the day\n","- `Day Calls`: the total number of calls placed during the day\n","- `Day Charge`: the billed cost of daytime calls\n","- `Eve Mins, Eve Calls, Eve Charge`: the billed cost for calls placed during the evening\n","- `Night Mins`, `Night Calls`, `Night Charge`: the billed cost for calls placed during nighttime\n","- `Intl Mins`, `Intl Calls`, `Intl Charge`: the billed cost for international calls\n","- `CustServ Calls`: the number of calls placed to Customer Service\n","- `Churn?`: whether the customer left the service: true/false\n","\n","The last attribute, `Churn?`, is known as the target attribute: the attribute that we want the ML model to predict.  Because the target attribute is binary, our model will be performing binary prediction, also known as binary classification."]},{"cell_type":"markdown","id":"3217f3c5","metadata":{"tags":[]},"source":["We have cleaned up the dataset to rid of one feature from each of the highly correlated pairs: `Day Charge` from the pair with `Day Mins`, `Night Charge` from the pair with `Night Mins`, `Intl Charge` from the pair with `Intl Mins`. This is to because including these feature pairs in some machine learning algorithms can create catastrophic problems, while in others it will only introduce minor redundancy and bias."]},{"cell_type":"markdown","id":"55c68929-b3e5-4b63-b7d6-491ec0ccbf58","metadata":{},"source":["We have randomly sampled 10% of the churn data for testing purposes and used the rest of the data for training and evaluating purposes."]},{"cell_type":"code","execution_count":null,"id":"a03f67ac","metadata":{},"outputs":[],"source":["DATASET_NAME = \"test.csv\"\n","data_source = f\"{source_bucket}/{data_prefix}/{DATASET_NAME}\"\n","print(\"test data: \")\n","!aws s3 cp $data_source ."]},{"cell_type":"code","execution_count":null,"id":"66eba932-06b3-4f47-8a3b-76efbad588f5","metadata":{},"outputs":[],"source":["test_data = pd.read_csv(\"test.csv\", header=None)\n","test_data.head(5)"]},{"cell_type":"markdown","id":"171515b0","metadata":{},"source":["---\n","## Using the endpoint\n","\n","Let's determine which algorithm to use. As mentioned above, there appear to be some variables where both high and low (but not intermediate) values are predictive of churn.  In order to accommodate this in an algorithm like linear regression, we'd need to generate polynomial (or bucketed) terms.  Instead, let's attempt to model this problem using gradient boosted trees.  Amazon SageMaker provides an XGBoost container that we can use to train in a managed, distributed setting, and then host as a real-time prediction endpoint.  XGBoost uses gradient boosted trees which naturally account for non-linear relationships between features and the target variable, as well as accommodating complex interactions between features.\n","\n","We have deployed the demo endpoint for you ."]},{"cell_type":"code","execution_count":null,"id":"8f0232f5","metadata":{},"outputs":[],"source":["xgb_predictor = sagemaker.predictor.Predictor(\n","    endpoint_name=endpoint_name,\n","    sagemaker_session=sess,\n","    serializer=CSVSerializer()\n",")"]},{"cell_type":"markdown","id":"29ab4cae","metadata":{},"source":["### Evaluate\n","\n","Now that we have a hosted endpoint running, we can make real-time predictions from our model very easily, simply by making a `http` POST request.  But first, we'll need to set up serializers and deserializers for passing our `test_data` NumPy arrays to the model behind the endpoint."]},{"cell_type":"markdown","id":"6f03c792","metadata":{},"source":["Now, we'll use a simple function to:\n","1. Loop over our test dataset\n","1. Split it into mini-batches of rows \n","1. Convert those mini-batchs to CSV string payloads\n","1. Retrieve mini-batch predictions by invoking the XGBoost endpoint\n","1. Collect predictions and convert from the CSV output our model provides into a NumPy array"]},{"cell_type":"code","execution_count":null,"id":"42d1317f","metadata":{},"outputs":[],"source":["def predict(data, rows=500):\n","    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n","    predictions = \"\"\n","    for array in split_array:\n","        predictions = \",\".join([predictions, xgb_predictor.predict(array).decode(\"utf-8\")])\n","    return np.fromstring(predictions[1:], sep=\",\")\n","\n","predictions = predict(test_data.to_numpy()[:, 1:])"]},{"cell_type":"code","execution_count":null,"id":"d825b78d-938f-494f-97e7-e102b58386cd","metadata":{},"outputs":[],"source":["print(predictions)"]},{"cell_type":"markdown","id":"54e86315","metadata":{},"source":["There are many ways to compare the performance of a machine learning model, but let's start by simply by comparing actual to predicted values.  In this case, we're simply predicting whether the customer churned (`1`) or not (`0`), which produces a confusion matrix."]},{"cell_type":"code","execution_count":null,"id":"6a5761a8-4a58-4c8d-a0ab-65a246527757","metadata":{},"outputs":[],"source":["pd.crosstab(\n","    index=test_data.iloc[:, 0],\n","    columns=np.round(predictions),\n","    rownames=[\"actual\"],\n","    colnames=[\"predictions\"],\n",")"]},{"cell_type":"markdown","id":"39682258-57be-4b81-abcc-13636dde677c","metadata":{},"source":["_Note, due to randomized elements of the algorithm, your results may differ slightly._\n","\n","Of the 247 churners, we've correctly predicted 236 of them (true positives). We also incorrectly predicted 18 customers would churn who then ended up not doing so (false positives).  There are also 11 customers who ended up churning, that we predicted would not (false negatives).\n","\n","An important point here is that because of the `np.round()` function above, we are using a simple threshold (or cutoff) of 0.5.  Our predictions from `xgboost` yield continuous values between 0 and 1, and we force them into the binary classes that we began with.  However, because a customer that churns is expected to cost the company more than proactively trying to retain a customer who we think might churn, we should consider lowering this cutoff.  That will almost certainly increase the number of false positives, but it can also be expected to increase the number of true positives and reduce the number of false negatives."]},{"cell_type":"markdown","id":"ce4a0e5b","metadata":{},"source":["---\n","## Extensions\n","\n","This notebook showcased how to use a pre-trained model that predicts whether a customer is likely to churn. There are several means of extending it including:\n","- Some customers who receive retention incentives will still churn.  Including a probability of churning despite receiving an incentive in our cost function would provide a better ROI on our retention programs.\n","- Customers who switch to a lower-priced plan or who deactivate a paid feature represent different kinds of churn that could be modeled separately.\n","- Modeling the evolution of customer behavior. If usage is dropping and the number of calls placed to Customer Service is increasing, you are more likely to experience churn then if the trend is the opposite. A customer profile should incorporate behavior trends.\n","- Actual training data and monetary cost assignments could be more complex.\n","- Multiple models for each type of churn could be needed.\n","\n","Regardless of additional complexity, similar principles described in this notebook are likely applied.\n","\n","**To see more details of end-to-end model training with hyper-parameter optimization and deployement using SageMaker, please click [xgboost_customer_churn.ipynb](./xgboost_customer_churn.ipynb) notebook.**"]}],"metadata":{"kernelspec":{"name":"python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"}},"nbformat":4,"nbformat_minor":5}