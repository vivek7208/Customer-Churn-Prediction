# ğŸ“ˆ Customer Churn Prediction with LightGBM, CatBoost, TabTransformer, and AutoGluon-Tabular in Amazon SageMaker ğŸš€

This repository contains a Jupyter notebook ğŸ““ that demonstrates how to use machine learning (ML) ğŸ§  for the automated identification of unhappy customers ğŸ˜, also known as customer churn prediction. The notebook employs Amazon SageMaker's implementation of [LightGBM](https://lightgbm.readthedocs.io/en/latest/) ğŸ’¡, [CatBoost](https://catboost.ai/) ğŸ±, [TabTransformer](https://arxiv.org/abs/2012.06678) ğŸ”„, and [AutoGluon-Tabular](https://auto.gluon.ai/stable/index.html) ğŸ“Š algorithms to train and host a customer churn prediction model using Amazon SageMaker's Automatic Model Tuning (AMT) ğŸ›ï¸.

## ğŸŒ Overview

Losing customers is costly for any business ğŸ’¼. Identifying unhappy customers early gives businesses a chance to offer incentives to stay ğŸ. However, ML models rarely give perfect predictions. This notebook showcases how to incorporate the relative costs of prediction mistakes when determining the financial outcome of using ML ğŸ’°.

The ML models used in this notebook are trained in two scenarios:

1ï¸âƒ£ Training a tabular model on the customer churn dataset with AMT.
2ï¸âƒ£ Using the trained tabular model to perform inference, i.e., classifying new samples.

In the end, the performance of the four models trained with AMT is compared on the same test data ğŸ§ª.

## ğŸ“– Notebook Contents

The notebook contains the following sections:

1ï¸âƒ£ **Set Up:** This part involves setting up the notebook and importing necessary libraries ğŸ“š.

2ï¸âƒ£ **Data Preparation and Visualization:** This section deals with preparing the dataset for the machine learning models and visualizing the data for a better understanding of it ğŸ“Š. 

3ï¸âƒ£ **Train A LightGBM Model with AMT:** This part involves training a LightGBM model with AMT ğŸ’¡.

4ï¸âƒ£ **Train A CatBoost model with AMT:** Similar to the previous section, this part focuses on the CatBoost model ğŸ±.

5ï¸âƒ£ **Train A TabTransformer model with AMT:** This section focuses on training a TabTransformer model with AMT ğŸ”„.

6ï¸âƒ£ **Train An AutoGluon-Tabular model:** This section is about training an AutoGluon-Tabular model ğŸ“Š.

7ï¸âƒ£ **Compare Prediction Results of Four Trained Models on the Same Test Data:** In the final section, the performance of the four trained models - LightGBM, CatBoost, TabTransformer, and AutoGluon-Tabular - is compared using the same test data ğŸ.

## ğŸ¤– Models Used

This notebook uses the following machine learning models:

### ğŸ’¡ LightGBM

[LightGBM](https://lightgbm.readthedocs.io/en/latest/) is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be distributed and efficient.

### ğŸ± CatBoost

[CatBoost](https://catboost.ai/) is an open-source gradient boosting on decision trees library with categorical features support.

### ğŸ”„ TabTransformer

[TabTransformer](https://arxiv.org/abs/2012.06678) is a model introduced in the paper "TabTransformer: Tabular Data Modeling Using Contextual Embeddings".

### ğŸ“Š AutoGluon-Tabular

[AutoGluon-Tabular](https://auto.gluon.ai/stable/index.html) is an automated machine learning model designed for tabular data.

## ğŸ“¸ Images

![image](https://github.com/vivek7208/Customer-Churn-Prediction/assets/65945306/f95c5cfa-13ef-4c4d-a2b1-b7230aefe97a)

|                         | LightGBM with AMT | CatBoost with AMT | TabTransformer with AMT | AutoGluon-Tabular |
|-------------------------|------------------:|------------------:|------------------------:|------------------:|
| Accuracy                |          0.895556 |          0.953333 |                0.960000 |          0.980000 |
| F1                      |          0.894855 |          0.953229 |                0.960526 |          0.980044 |
| AUC                     |          0.965412 |          0.991407 |                0.992040 |          0.997926 |

Each column represents a different model, and the rows represent different metrics (Accuracy, F1, and AUC) for each model ğŸ“ˆ.

## ğŸ§¹ Cleaning Up

To avoid incurring unnecessary charges, remember to delete the endpoint corresponding to the trained model. Instructions on how to do this are provided at the end of the notebook ğŸ§¾.

## ğŸ® Usage

You can use this notebook to evaluate the performance of LightGBM, CatBoost, TabTransformer, and AutoGluon-Tabular on your own dataset. The notebook was tested in Amazon SageMaker Studio on ml.t3.medium instance with Python 3 (Data Science) kernel ğŸ.
